{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYykZTsrbgVl7eh953PZJm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Transfer Learning with Tensorflow Part 2: Fine-tuning\n","\n","In the previous notebook, we covered transfer learning feature extraction, now it's time to learn about a new kind of tranfer learning: fine-tuning."],"metadata":{"id":"jNGhZNADkWTE"}},{"cell_type":"code","source":["#Check if we're using a GPU\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2PO5MbdlqVl","executionInfo":{"status":"ok","timestamp":1656880139271,"user_tz":240,"elapsed":35,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"102c9634-2f9e-4088-93d2-65da8014cf86"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jul  3 20:28:58 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["## Creating helper functions\n","\n","In the previous notebooks, we've created a bunch of helper functions now we could rewrite them all, however it is tedious"],"metadata":{"id":"083Khx6flvoz"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJ_fL1PjmJvV","executionInfo":{"status":"ok","timestamp":1656880139488,"user_tz":240,"elapsed":229,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"e20591b9-318e-4f15-9a77-6ca95d2fdc72"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-03 20:28:59--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2022-07-03 20:28:59 (37.5 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}]},{"cell_type":"code","source":["# Import helper functions we're going to use in this notebook\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"],"metadata":{"id":"GxCFZVF0mk26","executionInfo":{"status":"ok","timestamp":1656880143119,"user_tz":240,"elapsed":3635,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Let's get some data\n","\n","This time we're going to see how we can use the pretrained models within `tf.keras.applications` and apply them to our own problem (recognizing images of food).\n","\n","link: https://www.tensorflow.org/api_docs/python/tf/keras/applications"],"metadata":{"id":"I1qdP1oyn0t7"}},{"cell_type":"code","source":["# Get 10% of training data of the 10 classes of Food101\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","\n","unzip_data(\"10_food_classes_10_percent.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6rtAcVRoucf","executionInfo":{"status":"ok","timestamp":1656880146810,"user_tz":240,"elapsed":3702,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"328b4cf0-19af-4418-83a2-ced4ad2a1d42"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-03 20:29:02--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 173.194.202.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: ‘10_food_classes_10_percent.zip’\n","\n","10_food_classes_10_ 100%[===================>] 160.74M   180MB/s    in 0.9s    \n","\n","2022-07-03 20:29:03 (180 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n","\n"]}]},{"cell_type":"code","source":["# Check out how many images and subdirectories are in our dataset\n","walk_through_dir(\"10_food_classes_10_percent\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QY4coal-piUa","executionInfo":{"status":"ok","timestamp":1656880146812,"user_tz":240,"elapsed":26,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"8b60ae2b-9d4c-4645-a6dd-6593a33b196c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in '10_food_classes_10_percent'.\n","There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n","There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n"]}]},{"cell_type":"code","source":["# Create training and test directory paths\n","train_dir = \"10_food_classes_10_percent/train\"\n","test_dir = \"10_food_classes_10_percent/test\""],"metadata":{"id":"h6ix5GLhp27R","executionInfo":{"status":"ok","timestamp":1656880146813,"user_tz":240,"elapsed":13,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir, \n","                                                                            image_size=IMG_SIZE,\n","                                                                            label_mode=\"categorical\",\n","                                                                            batch_size=BATCH_SIZE)\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir, \n","                                                                image_size=IMG_SIZE,\n","                                                                label_mode=\"categorical\",\n","                                                                batch_size=BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2tLb7sUqKyZ","executionInfo":{"status":"ok","timestamp":1656880152120,"user_tz":240,"elapsed":5318,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"703ef30b-5887-4be0-ac3f-0666a68c47b2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 750 files belonging to 10 classes.\n","Found 2500 files belonging to 10 classes.\n"]}]},{"cell_type":"code","source":["train_data_10_percent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gw9FENkCrWTg","executionInfo":{"status":"ok","timestamp":1656880152453,"user_tz":240,"elapsed":349,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"c1bd7623-ce27-4231-b1d5-db3362407ccd"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Check out the class names of our dataset\n","train_data_10_percent.class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxpVTC46rb3a","executionInfo":{"status":"ok","timestamp":1656880152455,"user_tz":240,"elapsed":16,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"fce48df5-cda1-4cdc-c188-e0288fcd340e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['chicken_curry',\n"," 'chicken_wings',\n"," 'fried_rice',\n"," 'grilled_salmon',\n"," 'hamburger',\n"," 'ice_cream',\n"," 'pizza',\n"," 'ramen',\n"," 'steak',\n"," 'sushi']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# See an example of the batch of data\n","for images, labels in train_data_10_percent.take(1):\n","  print(images, labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEnkyLtdrnih","executionInfo":{"status":"ok","timestamp":1656880153587,"user_tz":240,"elapsed":1140,"user":{"displayName":"siddharth Baichoo","userId":"04771885315163660769"}},"outputId":"ccff5d4a-7f4a-445a-8738-ed4a436a3a1a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[2.35183670e+02 2.42882660e+02 2.27413269e+02]\n","   [2.39642853e+02 2.50765305e+02 2.34525513e+02]\n","   [2.35204071e+02 2.47198975e+02 2.33642868e+02]\n","   ...\n","   [2.34061218e+02 2.26642792e+02 1.02566269e+02]\n","   [2.33714294e+02 2.23622452e+02 1.00668373e+02]\n","   [2.37357147e+02 2.23357147e+02 1.00357140e+02]]\n","\n","  [[2.31020416e+02 2.47668365e+02 2.26193878e+02]\n","   [2.34352036e+02 2.54923477e+02 2.33137756e+02]\n","   [2.33168365e+02 2.54683685e+02 2.36326523e+02]\n","   ...\n","   [2.31612198e+02 2.22183594e+02 9.89132004e+01]\n","   [2.29224472e+02 2.15423447e+02 9.23571320e+01]\n","   [2.26765244e+02 2.10857071e+02 8.88111572e+01]]\n","\n","  [[2.29571426e+02 2.40500000e+02 2.19142853e+02]\n","   [2.35198975e+02 2.51025513e+02 2.30040817e+02]\n","   [2.28112244e+02 2.48040817e+02 2.30586731e+02]\n","   ...\n","   [2.21806076e+02 2.08142761e+02 8.61427689e+01]\n","   [2.18102036e+02 2.01459183e+02 7.96734619e+01]\n","   [2.18367325e+02 2.00153046e+02 7.67958984e+01]]\n","\n","  ...\n","\n","  [[2.11362274e+02 2.44290787e+02 1.62709320e+02]\n","   [2.07301102e+02 2.43785736e+02 1.54984818e+02]\n","   [2.05331711e+02 2.44265366e+02 1.44852020e+02]\n","   ...\n","   [2.19617386e+02 2.47525558e+02 1.36739822e+02]\n","   [2.18729599e+02 2.46943863e+02 1.36158127e+02]\n","   [2.19142914e+02 2.47357178e+02 1.37352005e+02]]\n","\n","  [[2.04306061e+02 2.33642807e+02 1.47260025e+02]\n","   [2.06918350e+02 2.39637741e+02 1.48413162e+02]\n","   [2.08214279e+02 2.46086700e+02 1.47301025e+02]\n","   ...\n","   [2.22571472e+02 2.49801041e+02 1.36642853e+02]\n","   [2.21347000e+02 2.47928558e+02 1.34780579e+02]\n","   [2.23096954e+02 2.47025513e+02 1.34168274e+02]]\n","\n","  [[1.94673370e+02 2.25397842e+02 1.13999687e+02]\n","   [2.00760101e+02 2.33688675e+02 1.29040665e+02]\n","   [2.03290680e+02 2.38351898e+02 1.45438660e+02]\n","   ...\n","   [2.23132767e+02 2.50000000e+02 1.34224442e+02]\n","   [2.24734756e+02 2.46928558e+02 1.30525452e+02]\n","   [2.26000000e+02 2.45872421e+02 1.30056046e+02]]]\n","\n","\n"," [[[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.53000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.52571426e+02 2.55000000e+02 2.54000000e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]\n","\n","  [[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.53000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.52571426e+02 2.55000000e+02 2.54000000e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]\n","\n","  [[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.53000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.52663269e+02 2.55000000e+02 2.54000000e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]\n","\n","  ...\n","\n","  [[2.53000000e+02 2.53000000e+02 2.53000000e+02]\n","   [2.48285706e+02 2.48285706e+02 2.48285706e+02]\n","   [2.35857147e+02 2.35857147e+02 2.35857147e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]\n","\n","  [[2.53000000e+02 2.53000000e+02 2.53000000e+02]\n","   [2.47352051e+02 2.47352051e+02 2.47352051e+02]\n","   [2.34785706e+02 2.34785706e+02 2.34785706e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]\n","\n","  [[2.53000000e+02 2.53000000e+02 2.53000000e+02]\n","   [2.47285706e+02 2.47285706e+02 2.47285706e+02]\n","   [2.33780609e+02 2.33780609e+02 2.33780609e+02]\n","   ...\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n","   [2.54000000e+02 2.54000000e+02 2.54000000e+02]]]\n","\n","\n"," [[[2.47586731e+02 1.90413269e+02 1.07285721e+02]\n","   [2.46428574e+02 1.90428574e+02 1.07428566e+02]\n","   [2.43928558e+02 1.87938782e+02 1.06862244e+02]\n","   ...\n","   [7.81530457e+01 4.00204430e+01 1.98724689e+01]\n","   [8.14285812e+01 4.58112411e+01 2.24489822e+01]\n","   [7.89030228e+01 4.57448730e+01 1.88468533e+01]]\n","\n","  [[2.43806122e+02 1.90545914e+02 1.08637756e+02]\n","   [2.43275497e+02 1.90000000e+02 1.09857140e+02]\n","   [2.38301010e+02 1.86714294e+02 1.07887756e+02]\n","   ...\n","   [9.00254517e+01 4.47959061e+01 1.63673096e+01]\n","   [8.56428452e+01 4.21428833e+01 1.50765181e+01]\n","   [8.82907867e+01 4.57857819e+01 1.65459499e+01]]\n","\n","  [[2.48061234e+02 1.98494904e+02 1.20704079e+02]\n","   [2.48857147e+02 2.00071426e+02 1.23285713e+02]\n","   [2.44239792e+02 1.96408157e+02 1.20668365e+02]\n","   ...\n","   [1.35458923e+02 8.02650604e+01 4.07191658e+01]\n","   [1.19153015e+02 6.42091599e+01 2.54387665e+01]\n","   [1.10285614e+02 5.69998970e+01 1.93570404e+01]]\n","\n","  ...\n","\n","  [[2.48357147e+02 2.44357147e+02 2.35357147e+02]\n","   [2.47984695e+02 2.43984695e+02 2.34984695e+02]\n","   [2.48954071e+02 2.44954071e+02 2.35954071e+02]\n","   ...\n","   [2.40877548e+02 2.32739822e+02 2.13693924e+02]\n","   [2.41056137e+02 2.33571472e+02 2.17556229e+02]\n","   [2.42000000e+02 2.33785736e+02 2.20357208e+02]]\n","\n","  [[2.46714294e+02 2.42714294e+02 2.33714294e+02]\n","   [2.48071442e+02 2.44071442e+02 2.35071442e+02]\n","   [2.47071442e+02 2.43071442e+02 2.34071442e+02]\n","   ...\n","   [2.42142822e+02 2.34658157e+02 2.18642914e+02]\n","   [2.42928558e+02 2.34790771e+02 2.24066345e+02]\n","   [2.42928558e+02 2.35928558e+02 2.26071442e+02]]\n","\n","  [[2.47158112e+02 2.43158112e+02 2.34158112e+02]\n","   [2.49311157e+02 2.45311157e+02 2.36311157e+02]\n","   [2.46857101e+02 2.42857101e+02 2.33857101e+02]\n","   ...\n","   [2.43209274e+02 2.34995010e+02 2.22127777e+02]\n","   [2.42785797e+02 2.35785797e+02 2.26591995e+02]\n","   [2.43714355e+02 2.36714355e+02 2.30714355e+02]]]\n","\n","\n"," ...\n","\n","\n"," [[[1.26428576e+01 5.64285707e+00 0.00000000e+00]\n","   [1.41347256e+01 7.13472557e+00 0.00000000e+00]\n","   [1.47568560e+01 7.32828474e+00 0.00000000e+00]\n","   ...\n","   [1.61450901e+01 5.14508915e+00 0.00000000e+00]\n","   [1.42901783e+01 5.29017830e+00 2.90178537e-01]\n","   [1.42901783e+01 5.29017830e+00 2.90178537e-01]]\n","\n","  [[1.29226723e+01 5.92267227e+00 0.00000000e+00]\n","   [1.53949299e+01 8.39492989e+00 4.35267806e-01]\n","   [1.61125641e+01 8.00000000e+00 0.00000000e+00]\n","   ...\n","   [1.70000000e+01 6.00000000e+00 0.00000000e+00]\n","   [1.68705349e+01 7.00000000e+00 1.56473219e+00]\n","   [1.64352684e+01 7.43526793e+00 1.56473219e+00]]\n","\n","  [[1.32857141e+01 6.28571415e+00 0.00000000e+00]\n","   [1.73990746e+01 1.03990755e+01 2.39907503e+00]\n","   [1.70000000e+01 8.00000000e+00 0.00000000e+00]\n","   ...\n","   [1.70000000e+01 6.00000000e+00 0.00000000e+00]\n","   [1.65491066e+01 5.54910755e+00 2.74553776e-01]\n","   [1.55491076e+01 6.54910755e+00 2.74553776e-01]]\n","\n","  ...\n","\n","  [[2.42899780e+01 1.05645142e+01 1.83905041e+00]\n","   [2.58374367e+01 1.21119738e+01 3.38650966e+00]\n","   [2.55894241e+01 9.13849735e+00 2.02093506e+00]\n","   ...\n","   [9.66808987e+00 6.66808987e+00 1.57147217e+00]\n","   [7.18348122e+00 4.18348122e+00 0.00000000e+00]\n","   [7.98889160e+00 2.98889160e+00 9.80581939e-02]]\n","\n","  [[2.14821777e+01 8.48217773e+00 2.17636108e+00]\n","   [1.91494255e+01 6.14942455e+00 9.32725966e-01]\n","   [1.84545708e+01 4.34199953e+00 0.00000000e+00]\n","   ...\n","   [8.67147923e+00 5.67147970e+00 1.12774515e+00]\n","   [8.38149929e+00 5.38149929e+00 5.24382710e-01]\n","   [8.71435547e+00 3.71435547e+00 3.57177734e-01]]\n","\n","  [[2.00150967e+01 5.30531740e+00 3.05317432e-01]\n","   [1.77812061e+01 3.07142830e+00 0.00000000e+00]\n","   [1.91383495e+01 5.22127008e+00 0.00000000e+00]\n","   ...\n","   [6.12779331e+00 3.12779307e+00 1.14019014e-01]\n","   [1.31211939e+01 8.41141605e+00 4.28703928e+00]\n","   [1.12178688e+01 4.50809145e+00 5.18304482e-02]]]\n","\n","\n"," [[[1.89379303e+02 1.36379303e+02 3.03793049e+01]\n","   [1.82899078e+02 1.29899078e+02 2.38990746e+01]\n","   [1.87197861e+02 1.36197861e+02 2.71978645e+01]\n","   ...\n","   [2.18333237e+02 1.37904709e+02 1.40236807e+01]\n","   [2.13857117e+02 1.33857117e+02 1.08571167e+01]\n","   [2.12714355e+02 1.32714355e+02 1.17143555e+01]]\n","\n","  [[1.82503036e+02 1.29503036e+02 2.52307091e+01]\n","   [1.82468109e+02 1.29468109e+02 2.34681129e+01]\n","   [1.87435593e+02 1.34893021e+02 2.79292088e+01]\n","   ...\n","   [2.17399353e+02 1.37649582e+02 1.11069860e+01]\n","   [2.14844391e+02 1.34844391e+02 1.01167059e+01]\n","   [2.14688782e+02 1.34688782e+02 1.19611063e+01]]\n","\n","  [[1.84403214e+02 1.31842957e+02 2.87224159e+01]\n","   [1.88825745e+02 1.36673798e+02 3.12877884e+01]\n","   [1.89011642e+02 1.36011642e+02 3.08911037e+01]\n","   ...\n","   [2.19082520e+02 1.39533951e+02 9.77453232e+00]\n","   [2.18950912e+02 1.39910873e+02 1.21742983e+01]\n","   [2.19879471e+02 1.40319199e+02 1.44397316e+01]]\n","\n","  ...\n","\n","  [[2.19640717e+02 1.95640717e+02 1.60920395e+02]\n","   [2.16872192e+02 1.92872192e+02 1.54935013e+02]\n","   [2.14931580e+02 1.91931580e+02 1.50037018e+02]\n","   ...\n","   [8.82250519e+01 7.90107880e+01 2.00107861e+01]\n","   [9.01453857e+01 8.11053619e+01 2.53687267e+01]\n","   [8.78305664e+01 7.78305664e+01 2.82651939e+01]]\n","\n","  [[2.22432632e+02 1.96432632e+02 1.61257599e+02]\n","   [2.19605011e+02 1.93731430e+02 1.56352188e+02]\n","   [2.19107315e+02 1.96107315e+02 1.53737106e+02]\n","   ...\n","   [8.39419861e+01 7.47568893e+01 1.60291691e+01]\n","   [8.88503723e+01 7.97789307e+01 2.49412651e+01]\n","   [8.33826981e+01 7.52076721e+01 2.68247757e+01]]\n","\n","  [[2.16895309e+02 1.92777115e+02 1.53897552e+02]\n","   [2.17116669e+02 1.93520172e+02 1.52869843e+02]\n","   [2.17767395e+02 1.94767395e+02 1.52767395e+02]\n","   ...\n","   [8.45029373e+01 7.55029373e+01 1.99270077e+01]\n","   [8.92565460e+01 7.95239334e+01 2.60302963e+01]\n","   [8.89173584e+01 7.91230698e+01 2.86989956e+01]]]\n","\n","\n"," [[[2.32025513e+02 1.80025513e+02 1.90255108e+01]\n","   [2.12102036e+02 1.63627548e+02 6.60203934e+00]\n","   [2.40153076e+02 1.94658173e+02 4.52193909e+01]\n","   ...\n","   [2.51494888e+02 2.50494888e+02 2.02923416e+02]\n","   [2.48831650e+02 2.47831650e+02 1.99831650e+02]\n","   [2.50158112e+02 2.49158112e+02 2.01158112e+02]]\n","\n","  [[2.45591843e+02 1.93591843e+02 3.16377583e+01]\n","   [2.26357147e+02 1.76556122e+02 1.54897938e+01]\n","   [2.35357132e+02 1.89826523e+02 3.22704086e+01]\n","   ...\n","   [2.51229568e+02 2.50229568e+02 2.04229568e+02]\n","   [2.49071457e+02 2.48071457e+02 2.02071457e+02]\n","   [2.51642822e+02 2.50642822e+02 2.03836639e+02]]\n","\n","  [[2.44581635e+02 1.93438782e+02 2.85816326e+01]\n","   [2.39413269e+02 1.91357147e+02 2.31275501e+01]\n","   [2.39096939e+02 1.92816330e+02 2.43418350e+01]\n","   ...\n","   [2.52571472e+02 2.51571472e+02 2.07234772e+02]\n","   [2.52801025e+02 2.51586731e+02 2.08229599e+02]\n","   [2.51142746e+02 2.49928467e+02 2.06571320e+02]]\n","\n","  ...\n","\n","  [[1.47561234e+02 5.67755051e+01 3.98976779e+00]\n","   [1.42627502e+02 5.10560379e+01 5.71384430e-01]\n","   [1.40908188e+02 5.06939087e+01 1.33676624e+00]\n","   ...\n","   [2.09596878e+02 1.23811150e+02 3.82618815e-01]\n","   [2.13081406e+02 1.29810944e+02 8.76496696e+00]\n","   [2.10387955e+02 1.28806305e+02 1.08063040e+01]]\n","\n","  [[1.43831650e+02 4.98316536e+01 0.00000000e+00]\n","   [1.41005112e+02 4.90051193e+01 0.00000000e+00]\n","   [1.43882629e+02 5.20969200e+01 3.52548909e+00]\n","   ...\n","   [2.17974365e+02 1.29989670e+02 3.94374371e+00]\n","   [2.17862213e+02 1.32795883e+02 7.92855835e+00]\n","   [2.10270386e+02 1.24933701e+02 1.60203898e+00]]\n","\n","  [[1.47071426e+02 5.30714302e+01 1.07142842e+00]\n","   [1.41642822e+02 4.96428223e+01 0.00000000e+00]\n","   [1.42653107e+02 5.08673973e+01 2.29596806e+00]\n","   ...\n","   [2.22765533e+02 1.35765533e+02 6.76553392e+00]\n","   [2.22653076e+02 1.34653076e+02 8.65306950e+00]\n","   [2.19025543e+02 1.31025543e+02 7.25515175e+00]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n","[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["## Model 0: Building a transfer learning model using the Keras Functional API\n","\n","The sequential API is straight-forward, it runs our layers in a sequential order.\n","\n","But the functional API gives us more flexiblility with our model"],"metadata":{"id":"8zNQkRcmsLyN"}},{"cell_type":"code","source":["# 1. Create base model with tf.keras.applications\n","base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n","\n","# 2. Freeze the base model (so the underlying pre-trained patterns aren't updated during training)\n","base_model.trainable=False\n","\n","# 3. Create inputs into our model\n","inputs = tf.keras.layers.Input(shape=(224,224,3), name=\"input_layer\")\n","\n","# 4. If using ResNet50V2 you will need to normalize inputs\n","# x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)\n","\n","# 5. Pass the inputs to the base_model\n","x = base_model(inputs)\n","print(f\"Shape after passing inputs through base model: {x.shape}\")\n","\n","# 6. Average pool the outputs of the base model (aggregate all most important information, reduce number of computations)\n","x = tf.keras.layers.GlobalAveragePooling2D(name = \"global_average_pooling_layer\")(x)\n","print(f\"Shape after GlobalAveragingPooling2D: {x.shape}\")\n","\n","# 7. Create the output activation layer\n","outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n","\n","# 8. Combine the inputs with the outputs into a model\n","model_0 = tf.keras.Model(inputs, outputs)\n","\n","# 9. Compile the model\n","model_0.compile(loss=\"categorical_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# 10. Fit the model\n","history_10_percent = model_0.fit(train_data_10_percent,\n","                                 epochs=5,\n","                                 steps_per_epoch=len(train_data_10_percent),\n","                                 validation_data=test_data,\n","                                 validation_steps=int(0.25*len(test_data)),\n","                                 callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n","                                                                        experiment_name=\"10_percent_feature_extraction\")])"],"metadata":{"id":"cwm4kHihAudS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a971bc08-e9d3-4bfa-f390-888876d543a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","16711680/16705208 [==============================] - 0s 0us/step\n","16719872/16705208 [==============================] - 0s 0us/step\n","Shape after passing inputs through base model: (None, 7, 7, 1280)\n","Shape after GlobalAveragingPooling2D: (None, 1280)\n","Saving TensorBoard log files to: transfer_learning/10_percent_feature_extraction/20220703-202918\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f52366815f0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/c_api_util.py\", line 53, in __del__\n","    def __del__(self):\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["# Evaluate on full test dataset\n","model_0.evaluate(test_data)"],"metadata":{"id":"QCu-hlMoFZEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the layers in our base model\n","for layer_number, layer in enumerate(base_model.layers):\n","  print(layer_number, layer.name) "],"metadata":{"id":"-9ZhaNYNFp4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summary of the base model\n","base_model.summary()"],"metadata":{"id":"PPbziZrRGIE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.summary()"],"metadata":{"id":"IOIKnfV6Gkff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out our model's training curves\n","plot_loss_curves(history_10_percent)"],"metadata":{"id":"7AH0QgEwG0sU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting a feature vector from a trained model\n","\n","Let's demonstrate the Global Average Pooling 2D layer\n","\n","We have a tensor after our model goes through `base_model` of shape (None, 7, 7, 1280).\n","\n","But then when it passes through GlobalAveragePooling2D, it turns into (None, 1280).\n","\n","Let's use a similar shaped tensor of (1 , 4, 4, 3) and then pass it to GlobalAveragePooling2D"],"metadata":{"id":"zESxrYZ_HbNQ"}},{"cell_type":"code","source":["# Define the input shape\n","input_shape = (1, 4, 4, 3)\n","\n","# Create a random tensor\n","tf.random.set_seed(42)\n","input_tensor = tf.random.normal(input_shape)\n","print(f\"Random input tensor:\\n {input_tensor}\\n\")\n","\n","# Pass the random tensor through a global average pooling 2D layer\n","global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n","print(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")"],"metadata":{"id":"qxXvpv4qI2ql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the shape of the different tensors\n","print(f\"Shape of the input tensor: {input_tensor.shape}\")\n","print(f\"Shape of the Global Average pooled 2D tensor: {global_average_pooled_tensor.shape}\")\n"],"metadata":{"id":"dKAPBLwoHtbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's replicate the GlobalAveragePool2D layer\n","tf.reduce_mean(input_tensor, axis=[1,2])"],"metadata":{"id":"qaEJKgVHL6k0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** One of the reasons feature extraction transfer learning is named how it is is because what often happens is pretrained model outputs a **feature vector** (a long tensor of numbers which represents the learned representation of the model on a particular sample, in our case, this is the output of the `tf.keras.layers.GlobalAveragePooling2D` layer) which can then be used to extract patterns out of for our own specific problem "],"metadata":{"id":"GE7-zQQKMZaq"}},{"cell_type":"markdown","source":["## Running a series of transfer learning experiments\n","\n","We've seen the incredible results tranfer learning can get with only 10% of the training data, but how does it go with 1% of the training data... how about we set up a bunch of experiments to find out:\n","\n","1. `model_1` - use feature extraction transfer learning with 1% of the training data with data augmentation\n","2. `model_2` - use feature extraction transfer learning with 10% of the training with data augmentation\n","3. `model_3` - use fine-tuning transfer learning on 10% of the training data with data augmentation\n","4. `model_4` - use fine-tuning transfer learning on 100% of the training data with data augmentation\n","\n","> **Note:** throughout all experiments the same test dataset will be used to evaluate our model... this ensures consistency across evaluation metrics  \n"],"metadata":{"id":"86JZkvllL6yr"}},{"cell_type":"code","source":["# Download and unzip data preprocessed\n","! wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n","unzip_data(\"10_food_classes_1_percent.zip\")"],"metadata":{"id":"qYs48ZD7_-Hs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Getting and preprocessing data for model_1"],"metadata":{"id":"vCjJl2a_G_-i"}},{"cell_type":"code","source":["# Create training and test dir\n","train_dir_1_percent = \"10_food_classes_1_percent/train\"\n","test_dir = \"10_food_classes_1_percent/test\""],"metadata":{"id":"a6RFOUdhA5ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How many images are we working with?\n","walk_through_dir(\"10_food_classes_1_percent\")"],"metadata":{"id":"4nEBnRRYGw6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up data loaders\n","IMG_SIZE = [224,224]\n","train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n","                                                                           label_mode = \"categorical\",\n","                                                                           image_size = IMG_SIZE,\n","                                                                           batch_size = BATCH_SIZE) # default = 32\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n","                                                                label_mode = \"categorical\",\n","                                                                image_size = IMG_SIZE,\n","                                                                batch_size = BATCH_SIZE)"],"metadata":{"id":"yTHObj_NHPdL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adding data augment\n","\n","To add data augmentation right into our models, we can use the layers inside:\n","\n","* `tf.keras.layers.experimental.preprocessing()`\n","\n","We can see the benefits of doing this within the Tensorflow Data augmentation documentation: https://www.tensorflow.org/tutorials/images/data_augmentation#use_keras_preprocessing_layers\n","\n","Off the top of our heads, after reading the docs, the benefits of using data augmentation inside the model are:\n","* Preprocessing of images (augmenting them) happens on the GPU (much faster) rather than the CPU.\n","* Image data augmentation only happens during training, so we can still export our whole model and use it elsewhere.\n","\n"],"metadata":{"id":"7o9pmbg2Kokz"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","# Create data augmentation stage with horizontal flipping, rotations, zooms, etc..\n","data_augmentation = keras.Sequential([\n","    preprocessing.RandomFlip(\"horizontal\"),\n","    preprocessing.RandomRotation(0.2),\n","    preprocessing.RandomZoom(0.2),\n","    preprocessing.RandomHeight(0.2),\n","    preprocessing.RandomWidth(0.2),\n","    preprocessing.Rescaling(1./255) # Keep for models like ResNet50V2 but efficientNet's having rescaling built in\n","], name = \"data_augmentation\")"],"metadata":{"id":"pWsbFMTNLjIo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### VIsualize out data augmentation layer (and see what happens to our data)\n"],"metadata":{"id":"7KM9JzL5j6DD"}},{"cell_type":"code","source":["# View a random image and compare it to its augmented version\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","import random\n","target_class = random.choice(train_data_1_percent.class_names)\n","target_dir = \"10_food_classes_1_percent/train/\" + target_class\n","random_image = random.choice(os.listdir(target_dir))\n","random_image_path = target_dir + \"/\" +random_image\n","\n","# Read in the random image\n","img = mpimg.imread(random_image_path)\n","plt.imshow(img)\n","plt.title(f\"Original random image from class {target_class}\")\n","plt.axis(False)\n","\n","# Now let's plot our augmented random image\n","augmented_img = data_augmentation(tf.expand_dims(img, axis=0), training=True)\n","plt.figure()\n","plt.imshow(tf.squeeze(augmented_img))\n","plt.title(f\"Augmented random image from class {target_class}\")\n","plt.axis(False)"],"metadata":{"id":"BGPZRLzikHVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 1: Feature extraction transfer learning on 1% of the data with data augmentation"],"metadata":{"id":"rDIEHao3yMIj"}},{"cell_type":"code","source":["from gc import callbacks\n","# Set up input shape and base model, freezing the base model layers \n","input_shape = (224, 224, 3)\n","base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n","base_model.trainable = False\n","\n","# Create input layer\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","\n","# Add in data augmentation Sequential model as a layer\n","x = data_augmentation(inputs)\n","\n","# Give base_model the inputs (after augmentation) and don't train it\n","x = base_model(x, training=False)\n","\n","# Pool output features of the base model\n","x = layers.GlobalAveragePooling2D(name= \"global_average_pooling_layer\")(x)\n","\n","# Put a dense layer on as the output\n","outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n","\n","# Make a model using the inputs and outputs\n","model_1 = keras.Model(inputs, outputs)\n","\n","# Compile the model\n","model_1.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])\n","\n","# Fit the model\n","history_1_percent = model_1.fit(train_data_1_percent,\n","                                epochs=5,\n","                                steps_per_epoch=len(train_data_1_percent),\n","                                validation_data=test_data,\n","                                validation_steps=int(0.25 * len(test_data)),\n","                                # Track the model training logs\n","                                callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n","                                                                       experiment_name=\"1_percent_data_aug\")])\n"],"metadata":{"id":"5kIjWSUY07ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.summary()"],"metadata":{"id":"jtprJbeOPTzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate on the full test dataset\n","result_1_percent_data_aug = model_1.evaluate(test_data)\n","result_1_percent_data_aug"],"metadata":{"id":"iMmjirRvPwbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss_curves(history_1_percent)"],"metadata":{"id":"0SvIDS1HQRtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 2: feature extraction transfer learning model with 10% of data and data augmentation"],"metadata":{"id":"FcSWJPJ7QYLf"}},{"cell_type":"code","source":["# Get 10% of data...\n","#!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","#unzip_data(10_food_classes_10_percent)\n","\n","train_dir_10_percent = \"10_food_classes_10_percent/train\"\n","test_dir = \"10_food_classes_10_percent/test\""],"metadata":{"id":"9k9ahF2uojQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How many images are in our directory\n","walk_through_dir(\"10_food_classes_10_percent\")"],"metadata":{"id":"rl2osGHTqTUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set data inputs\n","import tensorflow as tf\n","IMG_SIZE = (224, 224)\n","train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n","                                                                            label_mode = \"categorical\", \n","                                                                            image_size = IMG_SIZE)\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n","                                                                label_mode = \"categorical\", \n","                                                                image_size = IMG_SIZE)\n"],"metadata":{"id":"R8QaGBicphA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_augmentation.summary()"],"metadata":{"id":"A0i-iNn3qQqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create model 2 with data augmentation built in\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","from tensorflow.keras.models import Sequential\n","\n","# Build data augmentation layer\n","data_augmentation = Sequential([\n","    preprocessing.RandomFlip(\"horizontal\"),\n","    preprocessing.RandomHeight(0.2),\n","    preprocessing.RandomWidth(0.2),\n","    preprocessing.RandomZoom(0.2),\n","    preprocessing.RandomRotation(0.2)\n","    # preprocessing.Rescaling(1./255) # If you're using a model such as ResNet50V2, you'll need to rescale your data, efficientnet has rescaling built-in\n","], name = \"data_augmentation\")\n","\n","# Set up the input shape to our model\n","input_shape = (224, 224, 3)\n","\n","# Create a frozen base model (also called the backbone)\n","base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n","base_model.trainable = False\n","\n","# Create the inputs and outputs (including the layers in between)\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","x = data_augmentation(inputs) # augment our training images (augmentation doesn't occur on test data)\n","x = base_model(x, training=False) # pass augmented images to base model but keep it in the inference mode, this is also insures batchnorm layers don't get updated - https://keras.io/guides/transfer_learning/#build-a-model\n","x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_2D\")(x)\n","outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n","model_2 = tf.keras.Model(inputs, outputs)\n","\n","# Compile \n","model_2.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"lkL-ng_AqyQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2.summary()"],"metadata":{"id":"PdUDc-levmtE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.summary()"],"metadata":{"id":"vYHF0CSDvnEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a ModelCheckpoint callback\n","\n","The ModelCheckpoint callback intermediately saves our model (the full model or just the weights) during the training. This is useful so we can come and start where we left off "],"metadata":{"id":"2VoFMCeCxlpz"}},{"cell_type":"code","source":["# Set checkpoint path\n","checkpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\"\n","\n","# Create a model checkpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path,\n","                                                         save_weights_only = True,\n","                                                         save_best_only = False,\n","                                                         save_freq = \"epoch\", # Save every epoch\n","                                                         verbose=1)\n"],"metadata":{"id":"5M8H3jbiyiu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fit model 2 passing in the ModelCheckpoint callback"],"metadata":{"id":"sTuiypi4zwgg"}},{"cell_type":"code","source":["# Fit the model saving checkpoints evry epoch\n","initial_epochs = 5\n","history_10_percent_data_aug = model_2.fit(train_data_10_percent,\n","                                          epochs = initial_epochs,\n","                                          validation_data = test_data,\n","                                          validation_steps = int(0.25 * len(test_data)),\n","                                          callbacks=[create_tensorboard_callback(dir_name = \"transfer_learning\",\n","                                                                                 experiment_name = \"10_percent_data_aug\"),\n","                                                     checkpoint_callback])"],"metadata":{"id":"e-q1Z377z6Zm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.evaluate(test_data)"],"metadata":{"id":"wZDp-Ls31GZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check model_2 results on all test_data\n","results_10_percent_data_aug = model_2.evaluate(test_data)\n","results_10_percent_data_aug"],"metadata":{"id":"hOiT6bbjLe6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot model loss curves\n","plot_loss_curves(history_10_percent_data_aug)"],"metadata":{"id":"23_11N6UMJbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading in checkpointed weights\n","\n","Loading in checkpointed weights returns a model to a specific checkpoint."],"metadata":{"id":"JL-2Rj-YMoly"}},{"cell_type":"code","source":["# Load in saved model weights and evaluate model\n","model_2.load_weights(checkpoint_path)"],"metadata":{"id":"u0ApGXpNM1D7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate model 2 with loaded weights \n","loaded_weights_model_results = model_2.evaluate(test_data)"],"metadata":{"id":"FPYXmg7tNCoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If the results from our previously evaluated model_2 match the loaded weights, every \n","results_10_percent_data_aug == loaded_weights_model_results"],"metadata":{"id":"uaMxsk5ENhc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_10_percent_data_aug"],"metadata":{"id":"2xDO7L06ORRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_weights_model_results"],"metadata":{"id":"OQ8o8P1sRLwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check to see if loaded model results are very close to our previous non-loaded model results\n","import numpy as np\n","np.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results))"],"metadata":{"id":"huHuLAIRRQEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the difference between the 2 results\n","print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))"],"metadata":{"id":"gD3XPKudRups"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 3: Fine tuning an existing model on 10% of the data\n","\n","> **Note:** Fine-tuning usually works best *after* training a feature extraction model for a few epochs with large amounts of custom data\n","\n"],"metadata":{"id":"3ibgAqQHSCV8"}},{"cell_type":"code","source":["# Are these layers trainable?\n","for layer in model_2.layers:\n","  print(layer, layer.trainable)"],"metadata":{"id":"q-c_PStzSMCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What layers are in our base_model (EfficientNetB0) and are they trainable\n","for i, layer in enumerate(model_2.layers[2].layers):\n","  print(i, layer.name, layer.trainable)"],"metadata":{"id":"dvdGo7XlS25F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How many trainable variables are in our base model?\n","print(len(model_2.layers[2].trainable_variables))"],"metadata":{"id":"QBNIu5V8TO5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To begin fine-tuning, let's start by setting the last 10 layers of our base model.trainable\n","base_model.trainable = True\n","\n","# Freeze all layers except of the last 10\n","for layer in base_model.layers[:-10]:\n","  layer.trainable = False\n","\n","# Recompile (we have to recompile our models every time we make a change)\n"],"metadata":{"id":"eGg5nIQzTmnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile \n","model_2.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(lr = 0.0001), # When fine tuning you typically want to lower the learning rate by 10x\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"c-dgfeiaUQPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Note:** When using fine-tuning it's best practice to lower your learning rate by some amount. How much? This is a hyper parameter you can tune. But a good rule of thumb is at least 10x (though different sources will claim other values.) "],"metadata":{"id":"rQOPmplsVUAI"}},{"cell_type":"code","source":["# Check which layers are tunable (trainable)\n","for i, layer in enumerate(model_2.layers[2].layers):\n","  print(i, layer.name, layer.trainable)"],"metadata":{"id":"T02QgBSQU1gZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we've unfrozen some of the layers closer to the top, how many trainable variables do we have\n","print(len(model_2.trainable_variables)) "],"metadata":{"id":"Imq0bpblWLXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine tune for another 5 epochs\n","fine_tune_epochs = initial_epochs + 5\n","\n","# Refit the model (same as model_2 except with more trainable parameters)\n","history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent, \n","                                               epochs = fine_tune_epochs,\n","                                               validation_data = test_data,\n","                                               validation_steps = int(0.25 * len(test_data)),\n","                                               initial_epoch = history_10_percent_data_aug.epoch[-1], #start training from previous last epoch\n","                                               callbacks = [create_tensorboard_callback(dir_name = \"transfer_learning\",\n","                                                                                        experiment_name = \"10_percent_fine_tune_last_10\")])"],"metadata":{"id":"xZNO8s6ZWLei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the fine-tuned model (model_3 which is actually model_2 fine tuned for another 5 epochs)\n","results_fine_tune_10_percent = model_2.evaluate(test_data)"],"metadata":{"id":"1DzCo7GvWoeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out the loss curves of our fine-tuned model\n","plot_loss_curves(history_fine_10_percent_data_aug)"],"metadata":{"id":"Hr-oFXgWbQDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `plot_loss_curves` function works great with models which have only been fit once, however we want something to compare one series of running `fit()` with another (e.g before and after fine-tuning)"],"metadata":{"id":"WmFXnO9UbtkO"}},{"cell_type":"code","source":["# Let's create a function to compare traning histories\n","def compare_historys(original_history, new_history, initial_epochs = 5):\n","  \"\"\"\n","  Compares 2 Tensorflow history objects\n","  \"\"\"\n","  # Get original history measurements\n","  acc = original_history.history[\"accuracy\"]\n","  loss = original_history.history[\"loss\"]\n","\n","  \n","  val_acc = original_history.history[\"val_accuracy\"]\n","  val_loss = original_history.history[\"val_loss\"]\n","\n","  # Combine original history metrics with new_history metrics\n","  total_acc = acc + new_history.history[\"accuracy\"]\n","  total_loss = loss + new_history.history[\"loss\"]\n","\n","  total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n","  total_val_loss = val_loss + new_history.history[\"val_loss\"]\n","\n","  # Make plot for accuracy\n","  plt.figure(figsize = (8, 8))\n","  plt.subplot(2, 1, 1)\n","  plt.plot(total_acc, label = \"Training Accuracy\")\n","  plt.plot(total_val_acc, label = \"Val Accuracy\")\n","  plt.plot([initial_epochs - 1, initial_epochs - 1], plt.ylim(), label = \"Start Fine Tuning\")\n","  plt.legend(loc = \"lower right\")\n","  plt.title(\"Training and Validation Accuracy\")\n","\n","  # Make plot for loss\n","  plt.figure(figsize = (8, 8))\n","  plt.subplot(2, 1, 1)\n","  plt.plot(total_loss, label = \"Training Loss\")\n","  plt.plot(total_val_loss, label = \"Val Loss\")\n","  plt.plot([initial_epochs - 1, initial_epochs - 1], plt.ylim(), label = \"Start Fine Tuning\")\n","  plt.legend(loc = \"upper right\")\n","  plt.title(\"Training and Validation Loss\")"],"metadata":{"id":"JRKMvLKLciYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compare_historys(history_10_percent_data_aug,\n","                 history_fine_10_percent_data_aug,\n","                 initial_epochs=5)"],"metadata":{"id":"Peu77GxlfuGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 4: Fine tuning an existing model on all of the data\n"],"metadata":{"id":"Ch-WLdILjn4A"}},{"cell_type":"code","source":["# Download and unzip 10 classes of Food101 data with all images\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n","unzip_data(\"10_food_classes_all_data.zip\")"],"metadata":{"id":"aR-Zen9Qj0Ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up training and test dir\n","train_dir_all_data = \"10_food_classes_all_data/train\"\n","test_dir = \"10_food_classes_all_data/test\""],"metadata":{"id":"p_7Hkr26k00W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How many images are we working with now\n","walk_through_dir(\"10_food_classes_all_data\")"],"metadata":{"id":"b0fdL16rlvv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup data inputs\n","import tensorflow as tf\n","IMG_SIZE = (224, 224)\n","train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir_all_data,\n","                                                                                 label_mode = \"categorical\",\n","                                                                                 image_size = IMG_SIZE)\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n","                                                                label_mode = \"categorical\", \n","                                                                image_size = IMG_SIZE)"],"metadata":{"id":"N6pJzisnl8JJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The test dataset we've loaded in this is the same as what we've been using for previous experiments (all experiments have been used the same test dataset) \n","\n","Let's verify this..."],"metadata":{"id":"9AO-dTaAmZpU"}},{"cell_type":"code","source":["# Evaluate model 2 (this is fine-tuned on 10 percent of data version)\n","model_2.evaluate(test_data) "],"metadata":{"id":"UEHP7fzNm2VK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To train a fine-tuning model (model_4) we need to revert model_2 back to its feature extraction weights"],"metadata":{"id":"ITam2h9spjLY"}},{"cell_type":"code","source":["# Load weights from checkpoint, that way we can fine-tune from the same stage the 10 percent data model was fine-tuned from\n","model_2.load_weights(checkpoint_path) "],"metadata":{"id":"lwGneYIBpyX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's evaluate model_2 now\n","model_2.evaluate(test_data)"],"metadata":{"id":"lpswnr4rm2bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check to see if our model_2 has been reverted back to feature extraction results\n","results_10_percent_data_aug"],"metadata":{"id":"oWtIaefxq2IP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Alright, the previous steps might seem quite confusing but all we've done is:\n","1. Trained a feature extraction transfer learning model for 5 epochs on 10% of the data with data augmentation (model_2) and we saved the model's weights using `ModelCheckpoint` callback.\n","2. Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen (model_3)\n","3. Saved the results and training logs each time\n","4. Reloaded the model from step 1 to do the same steps as step 2 except this time we're going to use all of the data (model_4)"],"metadata":{"id":"RgKPWn2yrNaW"}},{"cell_type":"code","source":["# Check which layers are tunable in the whole model\n","for layer_number, layer in enumerate(model_2.layers):\n","  print(layer_number, layer.name, layer.trainable)"],"metadata":{"id":"9YK9AtBlscqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's drill into our base_model (efficientnetb0) and see what layers are trainable\n","for layer_number, layer in enumerate(model_2.layers[2].layers):\n","  print(layer_number, layer.name, layer.trainable)"],"metadata":{"id":"sisz_8grs0p8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile \n","model_2.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n","                metrics = [\"accuracy\"])\n","\n","history_fine_10_classes_full = model_2.fit(train_data_10_classes_full, \n","                                           epochs = fine_tune_epochs,\n","                                           validation_data = test_data,\n","                                           validation_steps = int(0.25 * len(test_data)),\n","                                           initial_epoch = history_10_percent_data_aug.epoch[-1], #start training from previous last epoch\n","                                           callbacks = [create_tensorboard_callback(dir_name = \"transfer_learning\",\n","                                                                                    experiment_name = \"full_10_classes_fine_tune_last_10\")])"],"metadata":{"id":"qX5qhP0XtKQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's evaluate on all of the test data\n","results_fine_tune_full_data = model_2.evaluate(test_data)\n","results_fine_tune_full_data"],"metadata":{"id":"CA035cZYuTxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How did fine-tuning go with more data?\n","compare_historys(original_history = history_10_percent_data_aug,\n","                 new_history = history_fine_10_classes_full)"],"metadata":{"id":"d-tgKwQsvvtI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Viewing our experiment data on TensorBoard\n","> **Note:** Anything you upload to TensorBoard.dev is going to be public. So if you have private data do not upload."],"metadata":{"id":"AogQzdktwM92"}},{"cell_type":"code","source":["# View tensorboard logs of transfer learning modelling experiments (should -4 models)\n","# Upload TensorBoard dev records \n","!tensorboard dev upload --logdir ./transfer_learning \\\n","  --name \"Tranfer Learning experiments with 10 Food101 classes\" \\\n","  --description \"A series of different transfer learning experiments with varying amounts of data and fine-tuning.\" \\\n","  --one_shot # exits the uploader once it's finished uploading  "],"metadata":{"id":"WmtPlAfhwvSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["My tensorboard experiments are available at https://tensorboard.dev/experiment/YZoS7ZsOR3Gha6wzu0MSWA/"],"metadata":{"id":"St9Nm3ZKyY-L"}},{"cell_type":"code","source":["!tensorboard dev list "],"metadata":{"id":"xvS31PlXyqZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To delete an experiment\n","#!tensorboard dev delete --experiment_id ARcrzL34QdynGQDXnq2dGQ"],"metadata":{"id":"c7OjQmxRyqkH"},"execution_count":null,"outputs":[]}]}